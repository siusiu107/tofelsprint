Narrator: Listen to part of a lecture in a statistics class.
Professor: I’m going to show you two numbers, and I want you to tell me a story. Department A: satisfaction 78%. Department B: satisfaction 71%. Which department is “better”?
Student: A?
Professor: That’s the obvious answer. Now—what if I tell you that within every year level—freshmen, sophomores, juniors—Department B scores higher?
Student: Wait, how can B be higher in every group but lower overall?
Professor: Exactly. That’s why averages can lie.
Professor: Here’s what happened: Department A has mostly seniors in the survey, and seniors are happier no matter where they are because they’re closer to graduating. Department B has lots of freshmen, and freshmen are stressed everywhere.
Student: So the composition changes the overall average.
Professor: Yes. You’re mixing apples and oranges and then blaming the fruit.
Professor: Let me pause—did I say “Department A is better”? No. I meant “the overall average makes it look better.”
Student: So what should we report?
Professor: You report stratified results. Or you adjust. Or, at minimum, you add a sentence that the sample is uneven.
Professor: One more twist: sometimes administrators love a single headline number. Our job is to resist the temptation.
Student: That seems… political.
Professor: It can be. But it’s also just good reasoning. If you ignore the groups, you may “fix” the wrong problem.
