Narrator: Listen to part of a lecture in a data science class.
Professor: Sort of, Imagine you’re looking at the same data twice and getting two different stories. You know, Today we're talking about A/B testing and false discovery rate. I'll toss around a couple terms—dataset shift, I guess, false discovery rate, overfitting—but I'll translate them as we go.
Student 1: I guess, Sorry—can I ask something? When you say dataset shift, is that basically bootstrap, or is it a different mechanism?
Professor: In other words, um, dataset shift is the process, and bootstrap is usually a proxy or a measurement for it. People mix them up because the plot looks clean, but the causality is messy.
Professor: Quick tangent: a common misconception students have every semester. That’s why confounder and p-value matter—those are constraints, you know, not trivia.
Professor: Uh, This is where controls and replication come in. Um, One control isolates false discovery rate; another isolates confounder. If both fail, uh, it’s not ‘bad luck’—it’s telling you the model assumptions are off.
Professor: Let’s do a mini-demo in our heads. Pretend you’re designing a measurement. If your instrument is sensitive to p-value but blind to variance, you’ll misread the system. So you either redesign the measurement, um, or you model the bias and quantify uncertainty.
Professor: So… what’s the catch here?
Professor: …Okay, that's fine. We'll do it together.
Student 2: So if we’re choosing between two explanations, what’s the fastest way to falsify one without running a whole new study?
Professor: L-Like Like—, Good. I guess, Look for an asymmetric prediction. Uh, Explanation A implies a change in variance even if false discovery rate stays constant; Explanation B doesn’t. Design a small test around that hinge point.
Professor: Anyway, um, to wrap up: A/B testing and false discovery rate is less about one fact and more about reasoning under constraints. Watch the proxies, watch the instruments, and don’t fall in love with one pretty plot.
